# -*- coding: utf-8 -*-
"""dpo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15CHDdhDwpLhPnvJkPJWtJgtXS3_w0czF
"""

!pip install unsloth unsloth_zoo

!pip install trl bitsandbytes peft transformers accelerate datasets

!pip show datasets

from unsloth import FastLanguageModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import DPOTrainer
from datasets import Dataset

base_model_name = "unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit"
fine_tuned_model_path = "vichitrarora/qwen_finetuned_sft"

tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)
tokenizer.pad_token = tokenizer.eos_token


base_model = AutoModelForCausalLM.from_pretrained(
    "unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit",
    load_in_4bit=True,
    device_map="auto"
)


fine_tuned_model = AutoModelForCausalLM.from_pretrained(
    fine_tuned_model_path,
    device_map="auto",
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

from peft import LoraConfig, get_peft_model
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
    bias="none"
)
fine_tuned_model = get_peft_model(fine_tuned_model, lora_config)

import pandas as pd
from datasets import Dataset


train_file_path = "/content/train_dpo_data_1600.csv"
eval_file_path = "/content/eval_dpo_data (1).csv"


train_df = pd.read_csv(train_file_path)
eval_df = pd.read_csv(eval_file_path)


train_df.rename(columns={
    "schema": "schema_info",
    "natural_language_query": "nl_query",
    "corret_mongo_query": "preferred_query",
    "incorrect_mongo_query": "rejected_query"
}, inplace=True)

eval_df.rename(columns={
    "schema": "schema_info",
    "natural_language_query": "nl_query",
    "corret_mongo_query": "preferred_query",
    "incorrect_mongo_query": "rejected_query"
}, inplace=True)


train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)

def format_for_dpo(example):
    return {
        "prompt": f"Schema: {example['schema_info']}\nQuery: {example['nl_query']}",
        "chosen": example["preferred_query"],
        "rejected": example["rejected_query"],
    }


train_dataset = train_dataset.map(format_for_dpo)
eval_dataset = eval_dataset.map(format_for_dpo)

from trl import DPOConfig
dpo_args = DPOConfig(
    output_dir="./dpo-output",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-5,
    num_train_epochs=3,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    fp16=True,
)

trainer = DPOTrainer(
    model=fine_tuned_model,           # Now explicitly using LoRA-applied policy model
    ref_model=base_model,
    args=dpo_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

trainer.train()

fine_tuned_model.save_pretrained("/content/lora-adapters")
tokenizer.save_pretrained("/content/lora-adapters")

fine_tuned_model = fine_tuned_model.merge_and_unload()


fine_tuned_model.save_pretrained("/content/merged-model")
tokenizer.save_pretrained("/content/merged-model")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "/content/merged-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

def generate_response(prompt, max_new_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response


prompt ="Schema: user(id,name,email)\nQuery: Write a MongoDB query to find users with email ending with '@gmail.com' "

response = generate_response(prompt=prompt, max_new_tokens=256)

print("Model Response:\n", response)

!pip install huggingface_hub
!huggingface-cli login

fine_tuned_model.push_to_hub("vichitrarora/qwen2.5_0.5B_dpo")
tokenizer.push_to_hub("vichitrarora/qwen2.5_0.5B_dpo")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def generate_mongo_query(nl_query, db_id, schema_info):
    model.eval()  # Set to inference mode

    prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Generate a MongoDB query from the given database schema and natural language request.

### Input:
Database ID: {db_id}
Schema: {schema_info}
Query: {nl_query}

### Response:
"""


    inputs = tokenizer(prompt, return_tensors="pt").to(device)


    with torch.no_grad():
        output_ids = model.generate(**inputs, max_length=512)

    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return generated_text


nl_query = "List all traffic events with critical severity."
db_id = "traffic_event"
schema_info = '"{ ""collections"": [{ ""name"": ""traffic_events"", ""document"": { ""properties"": { ""identifier"": { ""bsonType"": ""object"", ""properties"": { ""intersection_id"": {""description"": ""Filter by traffic junction""}, ""camera_id"": {""description"": ""Filter by surveillance unit""} } }, ""response"": { ""bsonType"": ""object"", ""properties"": { ""event"": { ""properties"": { ""severity"": {""description"": ""Critical, High, Low""}, ""type"": {""description"": ""Speeding, RedLightViolation, IllegalParking""}, ""vehicles"": { ""bsonType"": ""array"", ""items"": { ""properties"": { ""number_plate"": { ""bsonType"": ""string"", ""pattern"": ""^[A-Z]{2}[0-9]{2}[A-Z]{2}[0-9]{4}$"" }, ""speed"": {""bsonType"": ""double""}, ""timestamp"": {""bsonType"": ""date""}, ""location"": { ""bsonType"": ""array"", ""items"": {""bsonType"": ""double""} } } } } } } } } } } }], ""version"": 2 } "'

print("Generated MongoDB Query:\n", generate_mongo_query(nl_query, db_id, schema_info))







!nvidia-smi

import torch
torch.cuda.empty_cache()
torch.cuda.memory_summary(device=None, abbreviated=False)

!kill -9 $(nvidia-smi | awk '$2=="Processes:" {getline; while ($0 !~ /^[|]/) {print $5; getline}}')

